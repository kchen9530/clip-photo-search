# AI 照片搜索

一个使用 AI 技术通过自然语言查询搜索本地照片库的 Web 应用程序。系统使用 CLIP（对比语言-图像预训练）模型来理解图像和文本，实现跨照片的语义搜索。

## 功能特性

- 🔍 自然语言搜索查询（例如："海滩上的日落"、"玩耍的狗狗"）
- 🖼️ 带悬停效果的图片画廊
- ⚡ 使用 CLIP 嵌入实现快速语义搜索
- 📊 索引统计和重新索引功能
- 🎨 现代化、响应式 UI

## 为什么选择 CLIP 模型？

CLIP (Contrastive Language-Image Pre-training) 是 OpenAI 开发的多模态 AI 模型，特别适合照片搜索应用，原因如下：

1. **多模态理解**：CLIP 在同一个向量空间中同时理解图像和文本，使得文本查询能够直接匹配图像内容，无需额外的文本标注。

2. **语义理解**：不同于传统的基于标签或文件名的搜索，CLIP 能够理解图像的实际内容和场景，例如搜索"女人躺在海滩上"时，即使图片没有这些标签，也能找到相关照片。

3. **零样本学习**：CLIP 不需要针对特定数据集进行微调，可以直接理解各种自然语言描述，支持灵活的搜索查询。

4. **高效性**：ViT-B/32 版本在保持良好性能的同时，模型大小适中（~150MB），推理速度快，适合本地部署。

5. **开源且成熟**：CLIP 是开源模型，有活跃的社区支持，文档完善，易于集成。

6. **隐私保护**：所有处理都在本地完成，照片数据不会上传到云端，保护用户隐私。

## 架构

- **后端**：使用 CLIP 模型进行图像-文本匹配的 FastAPI
- **前端**：使用 Vite 进行快速开发的 React
- **AI 模型**：OpenAI CLIP (ViT-B/32) 用于语义理解

## 安装设置

### 前置要求

- Python 3.8+
- Node.js 16+
- 本地照片库目录

### 后端设置

1. 进入后端目录：
```bash
cd backend
```

2. 创建虚拟环境（推荐）：
```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

3. 安装依赖：
```bash
pip install -r requirements.txt
```

4. 设置照片库路径（可选，默认为 ~/Pictures）：
```bash
export PHOTO_LIBRARY_PATH="/path/to/your/photos"
```

5. 启动后端服务器：
```bash
python main.py
```

后端将：
- 加载 CLIP 模型（首次运行可能需要一分钟）
- 自动索引照片库中的所有图片
- 在 http://localhost:8000 启动 API 服务器

### 前端设置

1. 进入前端目录：
```bash
cd frontend
```

2. 安装依赖：
```bash
npm install
```

3. 启动开发服务器：
```bash
npm run dev
```

前端将在 http://localhost:3000 可用

## 使用方法

1. 同时启动后端和前端服务器
2. 等待初始索引完成（在 UI 中查看统计信息）
3. 输入自然语言搜索查询
4. 在画廊中浏览匹配的图片

### 示例查询

- "山上的日落"
- "微笑的人们"
- "桌上的食物"
- "猫和狗"
- "海滩度假"
- "婚礼仪式"
- "雪景"
- "女人躺在海滩上"
- "男人在唱歌"
- "玩耍的猫咪"

## API 端点

- `GET /` - API 状态
- `GET /health` - 健康检查
- `GET /stats` - 获取索引统计信息
- `POST /search` - 搜索图片
  ```json
  {
    "query": "你的搜索查询",
    "limit": 20
  }
  ```
- `POST /reindex` - 强制重新索引所有图片
- `GET /image` - 通过后端 API 提供图片服务（解决浏览器 file:// 协议限制）

## 配置

### 照片库路径

默认情况下，系统在 `~/Pictures` 中查找照片。你可以通过以下方式更改：

1. 设置环境变量：
```bash
export PHOTO_LIBRARY_PATH="/path/to/your/photos"
```

2. 或修改 `backend/main.py` 中的 `PHOTO_LIBRARY_PATH` 变量

### 支持的图片格式

- JPEG (.jpg, .jpeg)
- PNG (.png)
- GIF (.gif)
- BMP (.bmp)
- WebP (.webp)
- TIFF (.tiff, .tif)

## 性能说明

- 首次运行将下载 CLIP 模型（约 150MB）
- 初始索引可能需要一些时间，具体取决于图片数量
- 嵌入向量缓存在 `image_embeddings.npy` 和 `image_paths.json` 中
- 只有在添加新照片时才需要重新索引

## 索引文件说明

系统在索引图片时会生成两个缓存文件，用于加速搜索：

### `image_embeddings.npy`
- **类型**：NumPy 数组文件（二进制格式）
- **内容**：所有图片的 CLIP 嵌入向量
- **格式**：形状为 `(图片数量, 512)` 的二维数组
  - 每张图片对应一个 512 维的向量
  - 这个向量是图片的语义表示，由 CLIP 模型生成
- **用途**：存储图片的语义特征，用于快速计算相似度

### `image_paths.json`
- **类型**：JSON 文本文件
- **内容**：所有图片的完整路径列表
- **格式**：字符串数组，每个元素是一个图片的绝对路径
- **用途**：将向量索引映射回实际图片路径

### 工作原理

这两个文件是配对的，通过数组索引位置建立对应关系：

```
索引时：
图片 → CLIP模型 → 嵌入向量(512维) → 保存到 image_embeddings.npy[索引]
路径 → 保存到 image_paths.json[索引]

搜索时：
查询文本 → CLIP模型 → 查询向量(512维)
查询向量 vs 所有图片向量 → 计算相似度 → 找到最匹配的索引
索引 → 从 image_paths.json[索引] 获取路径 → 返回结果
```

### 为什么需要这两个文件？

1. **性能优化**：避免每次搜索都重新计算所有图片的嵌入向量，大幅提升搜索速度
2. **持久化存储**：索引一次，后续直接使用，无需重复处理
3. **配对关系**：通过数组索引位置将向量与路径一一对应，确保搜索结果准确

### 文件位置

这些文件默认保存在 `backend/` 目录下，可以通过 `.gitignore` 排除，避免提交到版本控制。

## 故障排除

### 前端图片无法加载

前端通过后端 API (`/image` 端点) 提供图片服务，避免了浏览器的 `file://` 协议限制。如果图片无法加载：

1. 检查浏览器控制台是否有错误
2. 确认后端服务器正在运行
3. 检查图片路径是否正确

### 模型加载问题

如果 CLIP 模型加载失败：
- 确保有足够的磁盘空间（模型约 500MB）
- 检查网络连接（首次下载）
- 验证 PyTorch 安装

### 索引失败

- 验证照片库路径是否存在
- 检查文件权限
- 确保图片是支持的格式

### 代理设置问题

如果遇到 SOCKS 代理相关错误：
- 在安装依赖时临时取消代理：`unset ALL_PROXY && pip install ...`
- 或使用国内镜像源：`pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ...`

## 测试图片

项目包含一个 `download_test_images.py` 脚本，可以下载测试图片用于测试搜索功能：

```bash
python3 download_test_images.py
```

脚本会下载包含人物、动物、场景等的多样化测试图片到 `test_photos/` 目录。

## 许可证

MIT
